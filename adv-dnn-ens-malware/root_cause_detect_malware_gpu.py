'''
export LD_LIBRARY_PATH=/home/jzhang2297/cuda/cuda/cuda-11.2/lib64:$LD_LIBRARY_PATH
usage python3 root_cause_detect_malware_gpu.py
we use combined selected test suite from all operators to examine the fault detection
'''

# Plotting keywords
plot_kwds = {'alpha': 0.15, 's': 80, 'linewidths': 0}
import matplotlib.pyplot as plt
import copy
import seaborn as sns
import time
def plot_clusters(data, algorithm, args, kwds, idr):
    start_time = time.time()
    labels = algorithm(*args, **kwds).fit_predict(data)
    if len(np.unique(labels)) == 1:
        return labels
    label_copy = copy.deepcopy(labels)
    label_list = list(label_copy)
    sorted_labels = sorted(label_copy)

    for i in range(-1,  np.array(sorted_labels).max() + 1):
        count = label_list.count(i)

    end_time = time.time()
    palette = sns.color_palette('deep', np.unique(labels).max() + 1)
    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]

    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds) # plot the first two dimensions
    frame = plt.gca()
    frame.axes.get_xaxis().set_visible(False)
    frame.axes.get_yaxis().set_visible(False)
    plt.title(f'Clusters found by {algorithm.__name__}', fontsize=24)
    plt.text(-0.5, 0.7, f'Clustering took {end_time - start_time:.2f} s', fontsize=14)
    plt.savefig(f'cluster_figs/cluster_plot_{metric}_bg{other_bg}_idr{idr}.png')
    plt.clf()
    return labels

# min-max rescale the last two features:
def scale_one(X):
    if len(np.unique(X)) == 1:
        if 0.0 not in np.unique(X):
            return X / X.max()
        else:
            return X
    else:
        return (X - X.min()) / (X.max() - X.min())
# Dimensionality Reduction
import hdbscan
import sklearn.metrics
import pandas as pd
import os
from cuml.manifold import UMAP
from sklearn.preprocessing import StandardScaler
def umap_gpu(ip_mat, min_dist, n_components, n_neighbors, metric):
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"

    scaler = StandardScaler()
    ip_std = scaler.fit_transform(ip_mat)
    reducer = UMAP(min_dist=min_dist, n_components=n_components, n_neighbors=n_neighbors, metric=metric)
    umap_embed = reducer.fit_transform(ip_std)

    return umap_embed

from DBCV.DBCV import DBCV
from scipy.spatial.distance import euclidean

import sys
import numpy as np

def readdata_np(data_path):
    try:
        data = np.load(data_path)
        return data
    except IOError as e:
        sys.stderr.write("Unable to open {0}.\n".format(data_path))
        sys.exit(1)

def root_cause_androzoo(dataset, model_name, adv_bg, other_bg, metric, other_operators, weighted, weights):
    best_results = []
    for idr in id_ratio:
        X, gt_lbls, pred_lbls = [], [], []
        for operator in adv_operators:
            if weighted==True:
                root_path = f'/Androzoo_test_suite/{model_name}/{operator}/{metric}/{weights}/{adv_bg}/{idr}/'
            else:
                root_path = f'/Androzoo_test_suite/{model_name}/{operator}/{metric}/{adv_bg}/{idr}/'
            path = root_path+'X_feat.data'
            y_gt_path = root_path+'y.data'
            y_pred_path = root_path+'y_pred.data'
            feat = readdata_np(path)
            y_gt = readdata_np(y_gt_path)
            y_pred = readdata_np(y_pred_path)
            X.append(feat)
            gt_lbls.append(y_gt)
            pred_lbls.append(y_pred)

        for operator in other_operators:
            if weighted==True:
                root_path = f'/Androzoo_test_suite/{model_name}/{operator}/{metric}/{weights}/{other_bg}/{idr}/'
            else:
                root_path = f'/Androzoo_test_suite/{model_name}/{operator}/{metric}/{other_bg}/{idr}/'
            path = root_path + 'X_feat.data'
            y_gt_path = root_path + 'y.data'
            y_pred_path = root_path + 'y_pred.data'
            feat = readdata_np(path)
            y_gt = readdata_np(y_gt_path)
            y_pred = readdata_np(y_pred_path)
            X.append(feat)
            gt_lbls.append(y_gt)
            pred_lbls.append(y_pred)

        # get the misprediction set
        mis_idx = np.concatenate(gt_lbls) != np.concatenate(pred_lbls)
        X = np.concatenate(X)[mis_idx]
        gt_lbls = np.concatenate(gt_lbls)[mis_idx]
        pred_lbls = np.concatenate(pred_lbls)[mis_idx]
        mispred = int(sum(mis_idx))
        bb, trace, hdbscan_in_umap, clustering_results = [], [], [], []
        if mispred <= 2:
            print(f'Insufficient mispredictions for idr={idr}, dataset={dataset}, model={model_name}, bg={adv_bg}')
            best_results.append(pd.DataFrame({"Number of Clusters": 0,
                                              "Silhouette Score": -1.0,
                                              "Combined Score": -1.0,
                                              "Number of Noisy Inputs": mispred,
                                              "Config": [-1, -1, -1, -1, -1.0],
                                              "id ratio": idr,
                                              "dataset": dataset,
                                              "metric": metric,
                                              "bg": adv_bg,
                                              "model": model}).iloc[0])

            continue
        suite_feat = []
        for sample, gt_label, pred_label in zip(X, gt_lbls, pred_lbls):
            concat_feat = np.concatenate((sample, [gt_label], [pred_label]))
            suite_feat.append(concat_feat)

        suite_feat = np.vstack(suite_feat)
        PY_scaled = scale_one(suite_feat[:, -1])
        suite_feat[:, -1] = PY_scaled
        TY_scaled = scale_one(suite_feat[:, -2])
        suite_feat[:, -2] = TY_scaled
        X_features = suite_feat[:, :-2]
        Sumn = 0

        if other_bg == 0.05:
            i_list, j_list = [150, 100, 60], [100, 50, 30]
        elif other_bg == 0.1:
            i_list, j_list = [150, 100, 60], [100, 50, 30]
        elif other_bg == 0.03:
            i_list, j_list = [100, 60], [50, 30]
        elif other_bg == 0.01:
            i_list, j_list = [60], [30]
        for i, j in zip(i_list, j_list):
            if i >= suite_feat.shape[0]:
                i = suite_feat.shape[0] - 5
                j = i - 10
                if j <= 0 or i <= 0:
                    i = suite_feat.shape[0] - 1
                    j = i - 1
            for k, o in zip([5, 10, 15, 20, 25], [3, 5, 10, 15, 20]):
                for n_n in [0.03, 0.1, 0.25, 0.5]:
                    # UMAP Dimensionality Reduction
                    u1 = umap_gpu(ip_mat=suite_feat, min_dist=n_n, n_components=i, n_neighbors=k, metric='Euclidean')
                    u = umap_gpu(ip_mat=u1, min_dist=0.1, n_components=j, n_neighbors=o, metric='Euclidean')
                    u = np.c_[u, TY_scaled, PY_scaled]

                    labels = plot_clusters(u, hdbscan.HDBSCAN, (), {'min_cluster_size': min_cluster_size}, idr)
                    config = [i, j, k, o, n_n]
                    if len(np.unique(labels))==1:
                        clustering_results.append({
                            "Number of Clusters": labels.max() + 1,
                            "Silhouette Score": -1,
                            "DBCV Score": -1,
                            "Combined Score": -1,  # 0.5 * silhouette_umap + 0.5 * DBCV_score,
                            "Number of Noisy Inputs": list(labels).count(-1),
                            "Config": config,
                            "id ratio": idr,
                            "dataset": dataset,
                            "metric": metric,
                            "bg": other_bg,
                            "model": model,
                        })
                    else:
                        # evaluate the clustering
                        silhouette_umap = sklearn.metrics.silhouette_score(u, labels)
                        silhouette_features = sklearn.metrics.silhouette_score(X_features, labels)


                        DBCV_score = DBCV(u, labels)


                        if (silhouette_umap >= 0.1 or silhouette_features >= 0.1): #and labels.max() + 2 >= 10:

                            Sumn += 1
                            # both scores closer to 1 indicate better clustering
                            clustering_results.append({
                                "Number of Clusters": labels.max() + 1,
                                "Silhouette Score": silhouette_umap,
                                "DBCV Score": DBCV_score,
                                "Combined Score": 0.5 * silhouette_umap + 0.5 * DBCV_score,
                                "Number of Noisy Inputs": list(labels).count(-1),
                                "Config": config,
                                "id ratio": idr,
                                "dataset": dataset,
                                "metric": metric,
                                "bg": other_bg,
                                "model": model,
                            })

                            print(f"Iteration {Sumn}: Noisy labels count: {list(labels).count(-1)}")

        # Display clustering results in a table and select the one config clustering that has best Silhouette/DBCV score
        clustering_df = pd.DataFrame(clustering_results)
        # Select the best clustering based on the combined score
        idx_clustering = np.argmax(clustering_df['Combined Score'])
        best_results.append(clustering_df.iloc[idx_clustering])
        print('best_results', best_results)
    if weighted == True:
        root_path = f'root_cause/{dataset}/{model_name}/{metric}/{weights}'
        if not os.path.exists(root_path):
            os.makedirs(root_path)
        pd.DataFrame(best_results).to_csv(root_path + f'bg{other_bg}extra.csv')
    else:
        root_path = f'root_cause/{dataset}/{model_name}/{metric}'
        if not os.path.exists(root_path):
            os.makedirs(root_path)
        pd.DataFrame(best_results).to_csv(root_path + f'bg{other_bg}extra.csv')

def root_cause_drebin(dataset, model_name, adv_bg, other_bg, metric, other_operators, weighted, weights):
    best_results = []
    for idr in id_ratio:
        X, gt_lbls, pred_lbls = [], [], []
        for operator in adv_operators:
            if weighted==True:
                root_path = f'/Drebin_test_suite/{model_name}/{operator}/{metric}/{weights}/{adv_bg}/{idr}/'
            else:
                root_path = f'/Drebin_test_suite/{model_name}/{operator}/{metric}/{adv_bg}/{idr}/'
            path = root_path+'X_feat.data'
            y_gt_path = root_path+'y.data'
            y_pred_path = root_path+'y_pred.data'
            feat = readdata_np(path)
            y_gt = readdata_np(y_gt_path)
            y_pred = readdata_np(y_pred_path)
            X.append(feat)
            gt_lbls.append(y_gt)
            pred_lbls.append(y_pred)

        for operator in other_operators:
            if weighted == True:
                root_path = f'/Drebin_test_suite/{model_name}/{operator}/{metric}/{weights}/{other_bg}/{idr}/'
            else:
                root_path = f'/Drebin_test_suite/{model_name}/{operator}/{metric}/{other_bg}/{idr}/'
            path = root_path + 'X_feat.data'
            y_gt_path = root_path + 'y.data'
            y_pred_path = root_path + 'y_pred.data'
            feat = readdata_np(path)
            y_gt = readdata_np(y_gt_path)
            y_pred = readdata_np(y_pred_path)
            X.append(feat)
            gt_lbls.append(y_gt)
            pred_lbls.append(y_pred)

        # get the misprediction set
        mis_idx = np.concatenate(gt_lbls) != np.concatenate(pred_lbls)
        X = np.concatenate(X)[mis_idx]
        gt_lbls = np.concatenate(gt_lbls)[mis_idx]
        pred_lbls = np.concatenate(pred_lbls)[mis_idx]
        mispred = int(sum(mis_idx))
        bb, trace, hdbscan_in_umap, clustering_results = [], [], [], []
        if sum(mis_idx) <= 2:

            best_results.append(pd.DataFrame({"Number of Clusters": 0,
                                              "Silhouette Score": -1.0,
                                              "Combined Score": -1.0,
                                              "Number of Noisy Inputs": mispred,
                                              "Config": [-1, -1, -1, -1, -1.0],
                                              "id ratio": idr,
                                              "dataset": dataset,
                                              "metric": metric,
                                              "bg": adv_bg,
                                              "model": model}).iloc[0])

            continue
        suite_feat = []
        for sample, gt_label, pred_label in zip(X, gt_lbls, pred_lbls):
            # add two extra features to capture actual and mispredicted classes (labels) related to each misclassified input.
            concat_feat = np.concatenate((sample, [gt_label], [pred_label]))
            suite_feat.append(concat_feat) # features of the misprediction set

        suite_feat = np.vstack(suite_feat)
        PY_scaled = scale_one(suite_feat[:, -1])
        suite_feat[:, -1] = PY_scaled
        TY_scaled = scale_one(suite_feat[:, -2])
        suite_feat[:, -2] = TY_scaled
        X_features = suite_feat[:, :-2]
        Sumn = 0

        if other_bg == 0.05:
            i_list, j_list = [150, 100, 60], [100, 50, 30]
        elif other_bg == 0.1:
            i_list, j_list = [150, 100, 60], [100, 50, 30]
        elif other_bg == 0.03:
            i_list, j_list = [100, 60], [50, 30]
        elif other_bg == 0.01:
            i_list, j_list = [60], [30]
        for i, j in zip(i_list, j_list):
            if i >= suite_feat.shape[0]:
                i = suite_feat.shape[0] - 5
                j = i - 10
                if j <= 0 or i <= 0:
                    i = suite_feat.shape[0] - 1
                    j = i - 1
            for k, o in zip([5, 10, 15, 20, 25], [3, 5, 10, 15, 20]):
                for n_n in [0.03, 0.1, 0.25, 0.5]:

                    # UMAP Dimensionality Reduction
                    u1 = umap_gpu(ip_mat=suite_feat, min_dist=n_n, n_components=i, n_neighbors=k, metric='Euclidean')
                    u = umap_gpu(ip_mat=u1, min_dist=0.1, n_components=j, n_neighbors=o, metric='Euclidean')
                    u = np.c_[u, TY_scaled, PY_scaled]

                    labels = plot_clusters(u, hdbscan.HDBSCAN, (), {'min_cluster_size': min_cluster_size}, idr)
                    config = [i, j, k, o, n_n]
                    if len(np.unique(labels))==1:
                        clustering_results.append({
                            "Number of Clusters": labels.max() + 1,
                            "Silhouette Score": -1,
                            "DBCV Score": -1,
                            "Combined Score": -1,  # 0.5 * silhouette_umap + 0.5 * DBCV_score,
                            "Number of Noisy Inputs": list(labels).count(-1),
                            "Config": config,
                            "id ratio": idr,
                            "dataset": dataset,
                            "metric": metric,
                            "bg": other_bg,
                            "model": model,
                        })
                    else:
                        # evaluate the clustering
                        silhouette_umap = sklearn.metrics.silhouette_score(u, labels)
                        silhouette_features = sklearn.metrics.silhouette_score(X_features, labels)
                        DBCV_score = DBCV(u, labels, dist_function=euclidean)

                        if (silhouette_umap >= 0.1 or silhouette_features >= 0.1): #and labels.max() + 2 >= 10:
                            Sumn += 1
                            # both scores closer to 1 indicate better clustering
                            clustering_results.append({
                                "Number of Clusters": labels.max() + 1,
                                "Silhouette Score": silhouette_umap,
                                "DBCV Score": DBCV_score,
                                "Combined Score": 0.5 * silhouette_umap + 0.5 * DBCV_score,
                                "Number of Noisy Inputs": list(labels).count(-1),
                                "Config": config,
                                "id ratio": idr,
                                "dataset": dataset,
                                "metric": metric,
                                "bg": other_bg,
                                "model": model,
                            })

                            print(f"Iteration {Sumn}: Noisy labels count: {list(labels).count(-1)}")

        # Display clustering results in a table and select the one config clustering that has best Silhouette/DBCV score
        clustering_df = pd.DataFrame(clustering_results)
        # Select the best clustering based on the combined score
        idx_clustering = np.argmax(clustering_df['Combined Score'])
        best_results.append(clustering_df.iloc[idx_clustering])
        print('best_results', best_results)
    if weighted == True:
        root_path = f'root_cause/{dataset}/{model_name}/{metric}/{weights}'
        if not os.path.exists(root_path):
            os.makedirs(root_path)
        pd.DataFrame(best_results).to_csv(root_path+f'bg{other_bg}extra.csv')
    else:
        root_path = f'root_cause/{dataset}/{model_name}/{metric}'
        if not os.path.exists(root_path):
            os.makedirs(root_path)
        pd.DataFrame(best_results).to_csv(root_path+f'bg{other_bg}extra.csv')
if __name__ == '__main__':
    params = {
            'dataset': ['Drebin'], #'Drebin',AndroZoo
            'model_name': ['basic_dnn', 'deepdrebin'], #'deepdrebin', basic_dnn
            'selection_metric': ['ensemble_simple_add'],
            'adv_budget': [0.05, 0.1, 0.2, 0.3],
            'other_budget': [0.01, 0.03, 0.05, 0.1],
        }
    id_ratio = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    adv_operators = ['fgsm', 'GDKDE', 'MIMICRY','POINTWISE']
    min_cluster_size = 5
    for dataset in params['dataset']:
        for model in params['model_name']:
            for adv_bg, other_bg in zip(params['adv_budget'], params['other_budget']):
                for metric in params['selection_metric']:
                    if dataset == 'Drebin':
                        other_operators = ['out-of-src']  # Drebin
                        root_cause_drebin(dataset, model, adv_bg, other_bg, metric, other_operators, True, '0.5_0.5_1.0')
                    elif dataset == 'AndroZoo':
                        other_operators = ['ad2018', 'ad2019', 'drebin']  # AndroZoo
                        root_cause_androzoo(dataset, model, adv_bg, other_bg, metric, other_operators, True, '0.5_0.5_60')
                    else:
                        other_operators = None
